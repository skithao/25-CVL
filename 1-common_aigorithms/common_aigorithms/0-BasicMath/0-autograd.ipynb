{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算导数和极限\n",
    "## 数学原理\n",
    "给定函数：\n",
    "$$ f(x) = 3x^2 + 2x + 1 $$\n",
    "根据导数的定义，函数在某点的导数为：\n",
    "$$ f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} $$\n",
    "将 $ f(x) = 3x^2 + 2x + 1 $ 代入上式：  \n",
    "\\begin{align*}\n",
    "f'(x) &= \\lim_{h \\to 0} \\frac{3(x + h)^2 + 2(x + h) + 1 - (3x^2 + 2x + 1)}{h}\\\\\n",
    "&= \\lim_{h \\to 0} \\frac{3(x^2 + 2xh + h^2) + 2x + 2h + 1 - 3x^2 - 2x - 1}{h}\\\\\n",
    "&= \\lim_{h \\to 0} \\frac{3x^2 + 6xh + 3h^2 + 2x + 2h + 1 - 3x^2 - 2x - 1}{h}\\\\\n",
    "&= \\lim_{h \\to 0} \\frac{6xh + 3h^2 + 2h}{h}\\\\\n",
    "&= \\lim_{h \\to 0} (6x + 3h + 2)\\\\\n",
    "&= 6x + 2\n",
    "\\end{align*}  \n",
    "所以，函数 $ f(x) $ 的导数为 $ f'(x) = 6x + 2 $。  \n",
    "## 代码原理  \n",
    "1. **定义函数**：\n",
    "    ```python\n",
    "    def function(x: tensor):\n",
    "        return 3 * x.pow(2) + 2 * x + 1\n",
    "    ```\n",
    "    此函数定义了 $ f(x) = 3x^2 + 2x + 1 $。输入参数 `x` 为一个 PyTorch 张量，函数会返回对应输入的函数值。\n",
    "\n",
    "2. **初始化计算图**：\n",
    "    ```python\n",
    "    x = torch.tensor(1.0, dtype=torch.float64, requires_grad=True)\n",
    "    y = function(x)\n",
    "    ```\n",
    "    - `torch.tensor(1.0, dtype=torch.float64, requires_grad=True)`：创建一个初始值为 1.0 的 PyTorch 张量 `x`，数据类型设定为 `float64`。`float64` 具有更高的精度，能减少在计算过程中因精度不足导致的误差。同时将 `requires_grad` 设为 `True`，表明后续需要对 `x` 求导。\n",
    "    - `y = function(x)`：调用 `function` 函数计算 `x` 对应的函数值 `y`。在这个过程中，PyTorch 会自动构建计算图，记录从 `x` 到 `y` 的所有操作。\n",
    "\n",
    "3. **自动反向传播**：\n",
    "    ```python\n",
    "    y.backward()\n",
    "    ```\n",
    "    `backward()` 方法会对计算图执行反向传播，计算 `y` 关于 `x` 的导数。反向传播完成后，`x` 的梯度值会存储在 `x.grad` 属性里。\n",
    "\n",
    "4. **输出理论导数和实际导数**：\n",
    "    ```python\n",
    "    print(f\"理论导数: {6*x + 2}\")\n",
    "    print(f\"实际导数: {x.grad}\")\n",
    "    ```\n",
    "    - 理论导数：依据数学公式 $ f'(x) = 6x + 2 $ 计算得出。\n",
    "    - 实际导数：通过 PyTorch 的自动求导功能计算得到，存储于 `x.grad` 中。\n",
    "\n",
    "5. **验证导数和极限**：\n",
    "    ```python\n",
    "    h = torch.tensor(1e-6, dtype=torch.float64)\n",
    "    approx_derivative = (function(x + h) - function(x)) / h\n",
    "    print(f\"近似导数 (使用极限定义): {approx_derivative}\")\n",
    "    ```\n",
    "    设定一个极小的 $ h $ 值（例如 $ 10^{-6} $），按照导数的极限定义 $ f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} $ 来近似计算导数，并将其与理论导数和实际导数进行对比。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "理论导数: 8.0\n",
      "实际导数: 8.0\n",
      "近似导数 (使用极限定义): 8.000002999608569\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "\n",
    "# 定义目标函数 f(x) = 3x^2 + 2x + 1\n",
    "def function(x: tensor):\n",
    "    # 以 3x^2 + 2x + 1 为例计算函数值\n",
    "    return 3 * x.pow(2) + 2 * x + 1 \n",
    "\n",
    "# 初始化计算图\n",
    "# 创建一个初始值为 1.0 的 PyTorch 张量 x，数据类型设定为 float64\n",
    "# float64 具有更高的精度，能减少在计算过程中因精度不足导致的误差\n",
    "# 将 requires_grad 设为 True，意味着后续计算中需要对 x 求导\n",
    "x = torch.tensor(1.0, dtype=torch.float64, requires_grad=True) \n",
    "# 调用 function 函数计算 x 对应的函数值 y\n",
    "# 此过程中，PyTorch 会自动构建计算图，记录从 x 到 y 的所有操作\n",
    "y = function(x)\n",
    "\n",
    "# 自动反向传播\n",
    "# backward() 方法会对计算图执行反向传播，计算 y 关于 x 的导数\n",
    "# 反向传播完成后，x 的梯度值会存储在 x.grad 属性里\n",
    "y.backward()  \n",
    "\n",
    "# 输出理论导数和实际导数\n",
    "# 理论导数依据数学公式 f'(x) = 6x + 2 计算得出\n",
    "print(f\"理论导数: {6*x + 2}\")\n",
    "# 实际导数通过 PyTorch 的自动求导功能计算得到，存储于 x.grad 中\n",
    "print(f\"实际导数: {x.grad}\")\n",
    "\n",
    "# 验证导数和极限\n",
    "# 设定一个极小的 h 值，用于近似导数的计算\n",
    "h = torch.tensor(1e-6, dtype=torch.float64)\n",
    "# 按照导数的极限定义 f'(x) = lim(h->0) [f(x+h)-f(x)] / h 来近似计算导数\n",
    "approx_derivative = (function(x + h) - function(x)) / h\n",
    "# 输出近似导数\n",
    "print(f\"近似导数 (使用极限定义): {approx_derivative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 计算梯度  \n",
    "\n",
    "## 数学原理  \n",
    "### 1. 函数定义  \n",
    "设向量 $\\mathbf{x} = [x_0, x_1, x_2, x_3]^T$，定义函数 $y = 2 \\cdot \\mathbf{x}^T \\mathbf{x}$。  \n",
    "向量点积展开为：  \n",
    "$$\n",
    "\\mathbf{x}^T \\mathbf{x} = x_0^2 + x_1^2 + x_2^2 + x_3^2\n",
    "$$  \n",
    "因此函数可表示为：  \n",
    "$$\n",
    "y = 2 \\cdot (x_0^2 + x_1^2 + x_2^2 + x_3^2)\n",
    "$$  \n",
    "\n",
    "### 2. 偏导数计算  \n",
    "对于函数 $y = 2x_i^2$（$i = 0, 1, 2, 3$），根据求导法则：  \n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial (2x_i^2)}{\\partial x_i} = 4x_i\n",
    "$$  \n",
    "即每个分量的偏导数为 $4x_i$。  \n",
    "\n",
    "### 3. 梯度计算  \n",
    "函数 $y$ 关于向量 $\\mathbf{x}$ 的梯度是各偏导数组成的向量：  \n",
    "$$\n",
    "\\nabla_{\\mathbf{x}} y = \\left[ \\frac{\\partial y}{\\partial x_0}, \\frac{\\partial y}{\\partial x_1}, \\frac{\\partial y}{\\partial x_2}, \\frac{\\partial y}{\\partial x_3} \\right]^T = [4x_0, 4x_1, 4x_2, 4x_3]^T = 4\\mathbf{x}\n",
    "$$  \n",
    "\n",
    "\n",
    "## 代码原理  \n",
    "### 1. 创建张量 `x`  \n",
    "```python  \n",
    "# 步骤 1: 创建一个张量 x  \n",
    "# 使用 torch.arange(4.0) 创建一个包含从 0 到 3 的浮点数的一维张量  \n",
    "x = torch.arange(4.0)  \n",
    "print(f\"创建的张量 x: {x}\")  \n",
    "```  \n",
    "- `torch.arange(4.0)` 生成一维张量 `x`，元素为 `[0.0, 1.0, 2.0, 3.0]`。  \n",
    "- `print` 语句输出张量内容，便于检查初始化结果。  \n",
    "\n",
    "\n",
    "### 2. 启用梯度计算  \n",
    "```python  \n",
    "# 步骤 2: 启用梯度计算  \n",
    "# 调用 requires_grad_(True) 方法，表明后续需要计算 x 的梯度  \n",
    "x.requires_grad_(True)  \n",
    "# 此时 x 的梯度尚未计算，所以其梯度值为 None  \n",
    "print(f\"x 的梯度初始值: {x.grad}\")  \n",
    "```  \n",
    "- `x.requires_grad_(True)` 激活张量 `x` 的梯度追踪功能，后续操作会记录计算图。  \n",
    "- 初始时 `x.grad` 为 `None`，因为未进行反向传播。  \n",
    "\n",
    "\n",
    "### 3. 定义函数 `y`  \n",
    "```python  \n",
    "# 步骤 3: 定义函数 y  \n",
    "# 计算 y = 2 * torch.dot(x, x)，这里 torch.dot 用于计算向量的点积  \n",
    "# 设 x = [x0, x1, x2, x3]，则 torch.dot(x, x) = x0² + x1² + x2² + x3²  \n",
    "# 所以 y = 2 * (x0² + x1² + x2² + x3²)  \n",
    "y = 2 * torch.dot(x, x)  \n",
    "print(f\"计算得到的函数值 y: {y}\")  \n",
    "```  \n",
    "- `torch.dot(x, x)` 计算向量自点积，结果为标量 $x_0^2 + x_1^2 + x_2^2 + x_3^2$。  \n",
    "- 乘以 2 得到函数值 $y$，并通过 `print` 输出验证。  \n",
    "\n",
    "\n",
    "### 4. 执行反向传播  \n",
    "```python  \n",
    "# 步骤 4: 执行反向传播  \n",
    "# 调用 backward() 方法，PyTorch 会自动计算 y 关于 x 的梯度  \n",
    "y.backward()  \n",
    "# 输出计算得到的 x 的梯度  \n",
    "print(f\"计算得到的 x 的梯度: {x.grad}\")  \n",
    "```  \n",
    "- `y.backward()` 触发反向传播算法，沿计算图逆序计算梯度。  \n",
    "- 梯度结果存储在 `x.grad` 中，输出结果应与理论推导一致（$4\\mathbf{x}$）。  \n",
    "\n",
    "\n",
    "### 5. 验证梯度计算结果  \n",
    "```python  \n",
    "# 步骤 5: 验证梯度计算结果  \n",
    "# 根据数学推导，y 关于 x 的梯度应该是 4 * x  \n",
    "# 这里验证计算得到的梯度是否等于 4 * x  \n",
    "print(f\"梯度计算结果是否正确: {x.grad == 4 * x}\")  \n",
    "```  \n",
    "- 理论梯度为 $4\\mathbf{x}$，即 `[0., 4., 8., 12.]`。  \n",
    "- 通过张量逐元素比较 `x.grad == 4 * x`，输出 `tensor([True, True, True, True])`，验证正确性。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建的张量 x: tensor([0., 1., 2., 3.])\n",
      "x 的梯度初始值: None\n",
      "计算得到的函数值 y: 28.0\n",
      "计算得到的 x 的梯度: tensor([ 0.,  4.,  8., 12.])\n",
      "梯度计算结果是否正确: tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 步骤 1: 创建一个张量 x\n",
    "# 使用 torch.arange(4.0) 创建一个包含从 0 到 3 的浮点数的一维张量\n",
    "x = torch.arange(4.0)\n",
    "print(f\"创建的张量 x: {x}\")\n",
    "\n",
    "# 步骤 2: 启用梯度计算\n",
    "# 调用 requires_grad_(True) 方法，表明后续需要计算 x 的梯度\n",
    "x.requires_grad_(True)\n",
    "# 此时 x 的梯度尚未计算，所以其梯度值为 None\n",
    "print(f\"x 的梯度初始值: {x.grad}\")\n",
    "\n",
    "# 步骤 3: 定义函数 y\n",
    "# 计算 y = 2 * torch.dot(x, x)，这里 torch.dot 用于计算向量的点积\n",
    "# 设 x = [x0, x1, x2, x3]，则 torch.dot(x, x) = x0^2 + x1^2 + x2^2 + x3^2\n",
    "# 所以 y = 2 * (x0^2 + x1^2 + x2^2 + x3^2)\n",
    "y = 2 * torch.dot(x, x)\n",
    "print(f\"计算得到的函数值 y: {y}\")\n",
    "\n",
    "# 步骤 4: 执行反向传播\n",
    "# 调用 backward() 方法，PyTorch 会自动计算 y 关于 x 的梯度\n",
    "y.backward()\n",
    "# 输出计算得到的 x 的梯度\n",
    "print(f\"计算得到的 x 的梯度: {x.grad}\")\n",
    "\n",
    "# 步骤 5: 验证梯度计算结果\n",
    "# 根据数学推导，y 关于 x 的梯度应该是 4 * x\n",
    "# 这里验证计算得到的梯度是否等于 4 * x\n",
    "print(f\"梯度计算结果是否正确: {x.grad == 4 * x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 求新的函数的梯度\n",
    "## 数学原理\n",
    "1. **第一个函数**\n",
    "    - 定义函数 $ y = \\sum_{i = 0}^{3} x_i=x_0 + x_1 + x_2 + x_3 $。\n",
    "    - 根据求导法则，对于函数 $ y $ 关于 $ x_i $ 的偏导数为：\n",
    "        - 当 $ i = 0,1,2,3 $ 时，$\\frac{\\partial y}{\\partial x_i}=1$。所以函数 $ y $ 关于向量 $ \\mathbf{x}=[x_0, x_1, x_2, x_3]^T $ 的梯度为 $ \\nabla_{\\mathbf{x}} y = [1, 1, 1, 1]^T $。\n",
    "2. **第二个函数**\n",
    "    - 定义函数 $ y = x_0 + 2x_1 $。\n",
    "    - 分别求关于 $ x_i $ 的偏导数：\n",
    "        - $\\frac{\\partial y}{\\partial x_0}=1$，$\\frac{\\partial y}{\\partial x_1}=2$，$\\frac{\\partial y}{\\partial x_2}=0$，$\\frac{\\partial y}{\\partial x_3}=0$。所以函数 $ y $ 关于向量 $ \\mathbf{x}=[x_0, x_1, x_2, x_3]^T $ 的梯度为 $ \\nabla_{\\mathbf{x}} y = [1, 2, 0, 0]^T $。\n",
    "## 代码原理\n",
    "1. **创建张量 `x`**\n",
    "    ```python\n",
    "    x = torch.tensor([0., 1., 2., 3.], requires_grad=True)\n",
    "    ```\n",
    "    创建一个包含从 0 到 3 的浮点数的一维张量 `x`，并将 `requires_grad` 设置为 `True`，表示后续需要计算 `x` 的梯度。\n",
    "\n",
    "2. **计算第一个函数的梯度**\n",
    "    ```python\n",
    "    y = x.sum()\n",
    "    y.backward()\n",
    "    print(f\"第一个函数计算得到的 x 的梯度: {x.grad}\")\n",
    "    ```\n",
    "        - `y = x.sum()`：定义第一个函数 `y` 为 `x` 中所有元素的和。\n",
    "        - `y.backward()`：执行反向传播，PyTorch 会自动计算 `y` 关于 `x` 的梯度。在第一次计算梯度时，由于 `x.grad` 初始值为 `None`，不需要清空梯度，所以去掉了原代码中的 `x.grad.zero_()`。\n",
    "        - `print(f\"第一个函数计算得到的 x 的梯度: {x.grad}\")`：输出计算得到的 `x` 的梯度。\n",
    "\n",
    "3. **计算第二个函数的梯度**\n",
    "    ```python\n",
    "    x.grad.zero_()\n",
    "    y = x[0] + 2 * x[1]\n",
    "    y.backward()\n",
    "    print(f\"第二个函数计算得到的 x 的梯度: {x.grad}\")\n",
    "    ```\n",
    "        - `x.grad.zero_()`：经过第一次梯度计算后，`x.grad` 已经有了值。为了避免第一次计算的梯度对本次计算产生影响，这里清空 `x` 的梯度，为计算新的函数梯度做准备。\n",
    "        - `y = x[0] + 2 * x[1]`：定义第二个函数 `y` 为 `x[0] + 2 * x[1]`。\n",
    "        - `y.backward()`：执行反向传播，计算 `y` 关于 `x` 的梯度。\n",
    "        - `print(f\"第二个函数计算得到的 x 的梯度: {x.grad}\")`：输出计算得到的 `x` 的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个函数计算得到的 x 的梯度: tensor([1., 1., 1., 1.])\n",
      "第二个函数计算得到的 x 的梯度: tensor([1., 2., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个包含从 0 到 3 的浮点数的一维张量 x，并启用梯度计算\n",
    "x = torch.tensor([0., 1., 2., 3.], requires_grad=True)\n",
    "\n",
    "# 第一步：计算第一个函数的梯度\n",
    "# 定义第一个函数 y 为 x 中所有元素的和\n",
    "# 即 y = x[0] + x[1] + x[2] + x[3]\n",
    "y = x.sum()\n",
    "# 执行反向传播，计算 y 关于 x 的梯度\n",
    "y.backward()\n",
    "# 输出计算得到的 x 的梯度\n",
    "print(f\"第一个函数计算得到的 x 的梯度: {x.grad}\")\n",
    "\n",
    "# 第二步：计算第二个函数的梯度\n",
    "# 清空 x 的梯度，为计算新的函数梯度做准备\n",
    "x.grad.zero_()\n",
    "# 定义第二个函数 y 为 y = x[0] + 2 * x[1]\n",
    "y = x[0] + 2 * x[1]\n",
    "# 执行反向传播，计算 y 关于 x 的梯度\n",
    "y.backward()\n",
    "# 输出计算得到的 x 的梯度\n",
    "print(f\"第二个函数计算得到的 x 的梯度: {x.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
